project: cannon-art
default_containers: 
  - dev
  - vnc
custom_cli_arguments:
  gpus:
    argparse:
      action: 'store_true'
    all:
      docker:
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  capabilities: [gpu]
      singularity:
        nv: true
  devices:
    argparse:
      action: 'store_true'
    all:
      devices:
        - /dev/video0
        - /dev/ttyACM0
        - /dev/ttyUSB0
  network_mode_host:
    argparse:
      action: 'store_true'
    all:
      network_mode: "host"
  singularity:
    argparse:
      action: 'store_true'
    all:
      singularity:
        sudo_override: False
        network:
          enable: true
          allocate_ip: false
          mode: "bridge"
        start:
          options:
            - contain
            - writable
        build:
          options:
            - force
            - sandbox
external:
  db:
    files:
      - name: "ATK-02-25-2022-04-30-30"
        id: "1rmO5F8yrIP61W409IxPI-ETzInRyV3rR"
        output: "@{project_root}/demos/bags/ATK-02-25-2022-04-30-30"
      - name: "ATK-02-25-2022-03-19-00"
        id: "1bHY8xFsCbtEuF2PTdwS7qdTPV0z479hZ"
        output: "@{project_root}/demos/bags/ATK-02-25-2022-03-19-00"
      - name: "pytorch-aarch64"
        id: "1XEvtAMeOAJQx9kQoRhVNdZt07L4y1d-D"
        output: "@{project_root}/containers/nx/scripts/pytorch/torch-1.9.0-python3.8-aarch64"
      - name: "torch-1.12cu11-aarch64"
        id: "1eOpFBNbP4L8_Ort5nmDIxqEJQcbluNdA"
        output: "@{project_root}/containers/nx/scripts/pytorch/torch-1.12cu11-aarch64"
services:
  dev: &dev_service
    image: "atk/@{project}:dev"
    hostname: "@{project}-dev"
    command: ""
    container_name: "@{project}-dev"
    entrypoint: ""
    build: &dev_build
      context: "./"
      dockerfile: "./containers/dev/dev.dockerfile"
      network: "host"
      args: &dev_build_args
        PROJECT: "@{project}"
        USER_UID: "@{uid}"
        USER_GID: "@{gid}"
        APT_DEPENDENCIES: "bash zsh vim git git-lfs python3-pip python3-tk python3-opencv rapidjson-dev ros-galactic-usb-cam"
        PIP_REQUIREMENTS: "wa_simulator pandas matplotlib numpy>=1.19 opencv-python tornado black Pillow torch torchvision pyserial nvidia-pyindex"
        USER_GROUPS: "dialout video"
    environment:
      DISPLAY: vnc:0.0
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "all"
    volumes: 
     - "@{project_root}:/home/@{container_username}/@{project}"
    tty: true
    working_dir: "/home/@{container_username}/@{project}/workspace"
  nx:
    <<: *dev_service
    image: "atk/@{project}:nx"
    hostname: "@{project}-nx"
    container_name: "@{project}-nx"
    network_mode: "host"
    build:
      <<: *dev_build
      args:
        <<: *dev_build_args
        CONTAINERNAME: "nx"
  vnc:
    image: "atk/@{project}:vnc"
    hostname: "@{project}-vnc"
    command: ""
    container_name: "@{project}-vnc"
    entrypoint: ""
    build:
      context: "./containers/vnc"
      dockerfile: "./vnc.dockerfile"
      network: "host"
      args:
        VNC_PASSWORD: "@{project}"
        RUN_XTERM: "yes"
        RUN_FLUXBOX: "yes"
    environment:
      RUN_XTERM: no
      RUN_FLUXBOX: yes
    ports:
      - "8080:8080"
      - "5900:5900"
  chrono:
    image: "atk/@{project}:chrono"
    hostname: "@{project}-chrono"
    command: ""
    container_name: "@{project}-chrono"
    entrypoint: ""
    build:
      context: ./containers/chrono
      dockerfile: ./chrono.dockerfile
      network: "host"
      args:
        USERNAME: "@{project}"
        USERHOME: "/home/@{container_username}"
        USERSHELL: "bash"
        USERSHELLPATH: "/bin/bash"
        USERSHELLPROFILE: "/home/@{container_username}/.bashrc"
        APT_DEPENDENCIES: "python3.8 python3.8-dev python3-pip libirrlicht-dev libnvidia-gl-515 libeigen3-dev git cmake cmake-curses-gui libglu1-mesa-dev mesa-common-dev swig freeglut3-dev libglfw3 libglfw3-dev x11proto-gl-dev libxxf86vm-dev openmpi-common libopenmpi-dev ninja-build python3-numpy"
        PIP_DEPENDENCIES: ""
    environment:
      USER_UID: "@{uid}"
      USER_GID: "@{gid}"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "all"
    ports:
      - "50000:50000"
    working_dir: "/home/@{container_username}/@{project}/sim"
    volumes: 
      - "@{project_root}:/home/@{container_username}/@{project}"
    tty: true
  pip-pychrono:
    image: "atk/@{project}:pip-pychrono"
    hostname: "@{project}-pip-pychrono"
    container_name: "@{project}-pip-pychrono"
    build:
      context: ./containers/pip-pychrono
      dockerfile: ./pip-pychrono.dockerfile
      network: "host"
      args:
        USERNAME: "@{project}"
        USERHOME: "/home/@{container_username}"
        USERSHELL: "bash"
        USERSHELLPATH: "/bin/bash"
        USERSHELLPROFILE: "/home/@{container_username}/.bashrc"
        APT_DEPENDENCIES: "libirrlicht-dev libnvidia-gl-495"
        CONDA_CHANNELS: "aryoung5 anaconda conda-forge"
        CONDA_DEPENDENCIES: "python=3.8.12 pip pychrono cudatoolkit anaconda-client conda-build"
        PIP_DEPENDENCIES: "numpy"
    environment:
      USER_UID: "@{uid}"
      USER_GID: "@{gid}"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "all"
    ports:
      - "50000:50000"
    working_dir: "/home/@{container_username}/@{project}/sim"
    volumes: 
      - "@{project_root}:/home/@{container_username}/@{project}"
    tty: true
networks:
  default:
    name: "@{project}"
